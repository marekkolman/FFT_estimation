{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Notebook contains full implementation of the FFT-based model estimation. Sample FX data is used for an example.\n",
    "Estimation implemented here covers GBM, VG, CGMY and $t$-distribution model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports\n",
    "Package `lmfit` is used for the minimization. Check https://lmfit.github.io/lmfit-py/ for more details.\n",
    "If using `Anaconda`, install `lmfit` by running `conda install lmfit` in your command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from lmfit import minimize, Minimizer, Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load FX data and compute log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data_FX.xlsx')\n",
    "data['prev_price'] = data['price'].shift(1)\n",
    "data['logret'] = data.eval('log(price/prev_price)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Definition of the Estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    '''\n",
    "        Object Estimator provides estimate of a stochatic model using FFT method\n",
    "    '''\n",
    "    N    = 2**16\n",
    "    eta  = 0.5\n",
    "    \n",
    "    dt   = 1/252\n",
    "    \n",
    "    def __init__(self, data_returns, N = None, eta = None, dt = None):\n",
    "        '''\n",
    "            Data returns: array-like of returns. NaNs will be removed\n",
    "            N           : (optional) number of integration points in the u-space\n",
    "            eta         : (optional) eta = delta u, i.e. step size of the integration\n",
    "            dt          : (optional) dt = h; time steps. If log returns are daily log returns then it is suggested to use h = 1/252\n",
    "            \n",
    "        '''\n",
    "        self.data_returns = pd.Series(data_returns).dropna()\n",
    "        \n",
    "        if N is not None:\n",
    "            self.N = N\n",
    "        \n",
    "        if eta is not None:\n",
    "            self.eta = eta\n",
    "        \n",
    "        if dt is not None:\n",
    "            self.dt  = dt\n",
    "            \n",
    "        self.make_fft_params()\n",
    "    \n",
    "    def make_fft_params(self):\n",
    "        '''\n",
    "            Function pepares FFT and integration variables\n",
    "        '''\n",
    "        self.b    = np.pi/self.eta\n",
    "        self.lbda = 2*np.pi/(self.eta * self.N)\n",
    "        \n",
    "        j         = np.arange(0, self.N, dtype = np.float64)\n",
    "        self.u    = j * self.eta\n",
    "        self.x    = -self.b + j * self.lbda   \n",
    "        \n",
    "        w         = np.ones_like(j)\n",
    "        w[0]      = 0.5\n",
    "        w[-1]     = 0.5\n",
    "        self.w    = w\n",
    "    \n",
    "    def get_cfXh(self, model):\n",
    "        '''\n",
    "            Function returns CF for X_h, i.e. CF for the random component in the model\n",
    "            \n",
    "            User is free to add CF for another model to the if statement below\n",
    "        '''\n",
    "        \n",
    "        if model.lower() == 'gbm':\n",
    "            return lambda params, u: np.exp(-0.5* params['sigma']**2 *self.dt* u**2)\n",
    "        elif model.lower() == 'vg':\n",
    "            return lambda params, u: (1- 1j*u*params['theta']*params['nu'] + 0.5 * params['sigma']**2 * u**2 * params['nu'])**(-self.dt / params['nu'])\n",
    "        elif model.lower() == 'cgmy':\n",
    "            from scipy.special import gamma\n",
    "            return lambda params, u: np.exp(params['C']* self.dt * gamma(-params['Y'])* ((params['M'] - 1j*u)**params['Y'] - params['M']**params['Y'] + (params['G']+1j*u)**params['Y'] - params['G']**params['Y']) )\n",
    "        elif model.lower() == 't':\n",
    "            from scipy.special import gamma, kv\n",
    "            modu = lambda params, u: np.abs(params['sigma'] * np.sqrt(self.dt * (params['nu']-2.0)) * u)\n",
    "            return lambda params, u: (kv(params['nu']/2.0, modu(params, u)) * modu(params, u) ** (params['nu']/2))/(gamma(params['nu']/2.0) * 2.0**(params['nu']/2.0-1.0))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_init_estim_params(self, model):\n",
    "        '''\n",
    "            Function returns 'initial guess' parameters for a given model\n",
    "            For exact details about how to specify the parameters, read: https://lmfit.github.io/lmfit-py/parameters.html\n",
    "        '''\n",
    "        \n",
    "        params = Parameters()\n",
    "        \n",
    "        if model.lower() == 'gbm':\n",
    "            params.add_many(('mu',    0.01, True, None,     None, None, None),\n",
    "                            ('sigma', 0.10, True, 0.00001,  None, None, None))\n",
    "        elif model.lower() == 'vg':\n",
    "            params.add_many(('mu',    0.01, True, None,     None, None, None),\n",
    "                            ('sigma', 0.10, True, 0.00001,  None, None, None),\n",
    "                            ('theta', 0.00, True, None,     None, None, None),\n",
    "                            ('nu',    0.01, True, 0.00001,  None, None, None))\n",
    "        elif model.lower() == 'cgmy':\n",
    "            params.add_many(('mu',    0.01, True, None,     None, None, None),\n",
    "                            ('C',     0.01, True, 0.00001,  None, None, None),\n",
    "                            ('G',     20,   True, 0.00001,  None, None, None),\n",
    "                            ('M',     20,   True, 0.00001,  None, None, None),\n",
    "                            ('Y',     1.85, True, None,     2.0, None, None))\n",
    "        elif model.lower() == 't':\n",
    "            params.add_many(('mu',    0.01, True, None,     None, None, None),\n",
    "                            ('sigma', 0.1,  True, 0.00001,  None, None, None),\n",
    "                            ('nu',    100,  True, 2.00001,  None, None, None))\n",
    "            self.u = np.clip(self.u, a_min = 0.001, a_max = None) #small override because CF evaluated at u=0 gives NaNs\n",
    "        return params\n",
    "    \n",
    "    def opt_fun(self, params, model):\n",
    "        '''\n",
    "            Implementation of MLE -LL(params) objective function (defined by means of the FFT) that will be minimized\n",
    "        '''\n",
    "        paramsdict = params.valuesdict()\n",
    "        u          = self.u\n",
    "        cfXh       = self.get_cfXh(model)\n",
    "        omega      = -1/self.dt * np.log(cfXh(paramsdict, -1j))\n",
    "        cf         = np.exp(1j*u * (paramsdict['mu']*self.dt + omega*self.dt))*cfXh(paramsdict, u)\n",
    "        \n",
    "        densities_fft = 1/np.pi * fft(np.exp(1j*self.b*u) * cf * self.eta * self.w)\n",
    "        densities     = np.clip(np.interp(self.data_returns, self.x, np.real(densities_fft)), a_min = 0.000000001, a_max = None)\n",
    "\n",
    "        return -np.sum(np.log(densities))\n",
    "    \n",
    "    def estimate(self, model, method = 'nelder'):\n",
    "        '''\n",
    "            Function execute the model fitting. By default it uses Nelder-Mead algorithm\n",
    "            \n",
    "            It uses lmfit package to find the optimal parameters minimizing -LL(params) funtion\n",
    "        '''\n",
    "        print(f'Estimating parameters of a {model} model')\n",
    "        params    = self.get_init_estim_params(model)\n",
    "        minimizer = Minimizer(self.opt_fun, params, fcn_args = (model,), nan_policy = 'omit')\n",
    "        minimizer.minimize(method = method)\n",
    "        return minimizer.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Estimate GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating parameters of a GBM model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Fit Statistics</h2><table><tr><td>fitting method</td><td>Nelder-Mead</td><td></td></tr><tr><td># function evals</td><td>92</td><td></td></tr><tr><td># data points</td><td>1</td><td></td></tr><tr><td># variables</td><td>2</td><td></td></tr><tr><td>chi-square</td><td> 1078129.48</td><td></td></tr><tr><td>reduced chi-square</td><td> 1078129.48</td><td></td></tr><tr><td>Akaike info crit.</td><td> 17.8907381</td><td></td></tr><tr><td>Bayesian info crit.</td><td> 13.8907381</td><td></td></tr></table><h2>Variables</h2><table><tr><th> name </th><th> value </th><th> initial value </th><th> min </th><th> max </th><th> vary </th></tr><tr><td> mu </td><td>  0.05616210 </td><td> 0.01 </td><td>        -inf </td><td>         inf </td><td> True </td></tr><tr><td> sigma </td><td>  0.07188569 </td><td> 0.1 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr></table>"
      ],
      "text/plain": [
       "<lmfit.minimizer.MinimizerResult at 0x2565d519f88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Estimator(data['logret']).estimate('GBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Estimate Variance Gamma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating parameters of a VG model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Fit Statistics</h2><table><tr><td>fitting method</td><td>Nelder-Mead</td><td></td></tr><tr><td># function evals</td><td>242</td><td></td></tr><tr><td># data points</td><td>1</td><td></td></tr><tr><td># variables</td><td>4</td><td></td></tr><tr><td>chi-square</td><td> 1103004.47</td><td></td></tr><tr><td>reduced chi-square</td><td> 1103004.47</td><td></td></tr><tr><td>Akaike info crit.</td><td> 21.9135484</td><td></td></tr><tr><td>Bayesian info crit.</td><td> 13.9135484</td><td></td></tr></table><h2>Variables</h2><table><tr><th> name </th><th> value </th><th> initial value </th><th> min </th><th> max </th><th> vary </th></tr><tr><td> mu </td><td>  0.05395687 </td><td> 0.01 </td><td>        -inf </td><td>         inf </td><td> True </td></tr><tr><td> sigma </td><td>  0.07087992 </td><td> 0.1 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr><tr><td> theta </td><td> -0.03926105 </td><td> 0.0 </td><td>        -inf </td><td>         inf </td><td> True </td></tr><tr><td> nu </td><td>  0.00235043 </td><td> 0.01 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr></table>"
      ],
      "text/plain": [
       "<lmfit.minimizer.MinimizerResult at 0x2565d20cf88>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Estimator(data['logret']).estimate('VG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Estimate CGMY model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating parameters of a CGMY model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Fit Statistics</h2><table><tr><td>fitting method</td><td>Nelder-Mead</td><td></td></tr><tr><td># function evals</td><td>1199</td><td></td></tr><tr><td># data points</td><td>1</td><td></td></tr><tr><td># variables</td><td>5</td><td></td></tr><tr><td>chi-square</td><td> 1108045.80</td><td></td></tr><tr><td>reduced chi-square</td><td> 1108045.80</td><td></td></tr><tr><td>Akaike info crit.</td><td> 23.9181085</td><td></td></tr><tr><td>Bayesian info crit.</td><td> 13.9181085</td><td></td></tr></table><h2>Variables</h2><table><tr><th> name </th><th> value </th><th> initial value </th><th> min </th><th> max </th><th> vary </th></tr><tr><td> mu </td><td>  0.05637064 </td><td> 0.01 </td><td>        -inf </td><td>         inf </td><td> True </td></tr><tr><td> C </td><td>  6.4544e-04 </td><td> 0.01 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr><tr><td> G </td><td>  4.99195403 </td><td> 20 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr><tr><td> M </td><td>  29.9719076 </td><td> 20 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr><tr><td> Y </td><td>  1.84945020 </td><td> 1.85 </td><td>        -inf </td><td>  2.00000000 </td><td> True </td></tr></table>"
      ],
      "text/plain": [
       "<lmfit.minimizer.MinimizerResult at 0x2565e668bc8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Estimator(data['logret']).estimate('CGMY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 $t$-distribution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating parameters of a t model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Fit Statistics</h2><table><tr><td>fitting method</td><td>Nelder-Mead</td><td></td></tr><tr><td># function evals</td><td>294</td><td></td></tr><tr><td># data points</td><td>1</td><td></td></tr><tr><td># variables</td><td>3</td><td></td></tr><tr><td>chi-square</td><td> 1105695.52</td><td></td></tr><tr><td>reduced chi-square</td><td> 1105695.52</td><td></td></tr><tr><td>Akaike info crit.</td><td> 19.9159851</td><td></td></tr><tr><td>Bayesian info crit.</td><td> 13.9159851</td><td></td></tr></table><h2>Variables</h2><table><tr><th> name </th><th> value </th><th> initial value </th><th> min </th><th> max </th><th> vary </th></tr><tr><td> mu </td><td>  0.05470736 </td><td> 0.01 </td><td>        -inf </td><td>         inf </td><td> True </td></tr><tr><td> sigma </td><td>  0.07043436 </td><td> 0.1 </td><td>  1.0000e-05 </td><td>         inf </td><td> True </td></tr><tr><td> nu </td><td>  5.98222229 </td><td> 100 </td><td>  2.00001000 </td><td>         inf </td><td> True </td></tr></table>"
      ],
      "text/plain": [
       "<lmfit.minimizer.MinimizerResult at 0x2565e67f948>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Estimator(data['logret']).estimate('t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
